{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nevralt nettverk for tekstklassifisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI og maskinlæringen er hyppig brukte ord i media og jeg en stund vært interessert i å lære mer om hvordan det egentlig fungerer. Bakgrunnen for prosjektet er dermed egeninteresse for å forstå de grunnleggende ideene bak maskinlæringsalgoritmer ved å selv implementere noe fra bunnen av. Jeg valgte å se på et nevralt nettverk, sterkt inspirert av youtube-serien om emnet av 3Blue1Brown. \n",
    "\n",
    "Jeg er interessert i språk og syntes ideen om å bruke AI til å analysere naturlig språk var spennnede. I tillegg var jeg interessert i å lære mer om ulike verktøy for tekstbehandling. Dermed valgte jeg å bruke det nevrale nettverket til tekstklassifisering. \n",
    "\n",
    "Koden er egenskrevet, men jeg har lest om teorien bak og hentet inspirasjon til kode fra ulike kilder. For det nevrale nettverket har <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\">kapittel 2</a> av Michael Nielsens gratis online bok om Nevralte nettverk vært til stor hjelp. For tekstanalysen har jeg delvis basert meg på <a href = \"https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6\">denne </a> artikkelen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "stemmer = LancasterStemmer()\n",
    "stopWords = set(stopwords.words('english'))\n",
    "datapath = '/../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funksjoner for å renske tekst-input og vektorisere datasett**\n",
    "\n",
    "\n",
    "Jeg har i første omgang brukt \"bag-of-words\" til å vektorisere tekststrenger. Det vil si at hvert tekststreng representeres av en vektor av 0-er og 1-ere: for hvert ord i en forhåndsdefinert base-vektor vil vektoren ha enten en 0 dersom orden ikke finnes i tekststrengen, eller et 1-tall dersom ordet finnes der. Senere i oppgaven tester jeg om jeg får bedre presisjon ved å bruke en mer avansert form for vektorisering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a string and returns a list of stemmed lowercase words\n",
    "def clean_text(t):\n",
    "    t_clean = [stemmer.stem(word) for word in (t.lower()).split()]\n",
    "    for i in range(0, len(t_clean)):\n",
    "        t_clean[i] = ''.join(filter(str.isalnum, t_clean[i]))\n",
    "    return t_clean\n",
    "\n",
    "\n",
    "#creates a vector of words and a vector of labels to use as a basis for vectorising text and labels\n",
    "def create_basises(training_data):\n",
    "    wordvec_basis = []\n",
    "    labels = []\n",
    "    for el in training_data:\n",
    "        labels.append(el[\"class\"])\n",
    "        t_clean = clean_text(el[\"sentence\"])\n",
    "        for w in t_clean:\n",
    "            if w not in stopWords: \n",
    "                wordvec_basis.append(w)\n",
    "    return list(set(wordvec_basis)), list(set(labels))\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(t, words):\n",
    "    t_clean = clean_text(t)\n",
    "    t_vec = []\n",
    "    for w in words:\n",
    "        if w in t_clean:\n",
    "            t_vec.append(1)\n",
    "        else:\n",
    "            t_vec.append(0)\n",
    "    t_vec = np.array(t_vec)\n",
    "    return t_vec\n",
    "\n",
    "#returns an array of 0s and 1s according to the label, based on label_basis\n",
    "def label_to_vec(l, label_basis):\n",
    "    l_vec = np.zeros(len(label_basis))\n",
    "    for i in range(0, len(label_basis)):\n",
    "        if l == label_basis[i]:\n",
    "            l_vec[i] += 1\n",
    "    return l_vec\n",
    "\n",
    "#vectorises a dataset(list of dictionaries giving sentence and class of each element)\n",
    "def vectorize_dataset(data, word_basis, label_basis):\n",
    "    text = []\n",
    "    label = []\n",
    "    for el in data:\n",
    "        t_vec = bow(el[\"sentence\"], word_basis)\n",
    "        l_vec = label_to_vec(el[\"class\"], label_basis)\n",
    "        text.append(t_vec)\n",
    "        label.append(l_vec)\n",
    "    return np.array(text), np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hjelpefunksjoner og funksjoner for å teste og trene det nevrale netverket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the sigmoid function\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "#Intitializes empty arrays for storing weights and biases based on a given shape for the neural network\n",
    "def empty_wb(shape):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(shape)-1):\n",
    "        weights.append(np.zeros((shape[i+1], shape[i])))\n",
    "        biases.append(np.zeros(shape[i+1]))\n",
    "    return weights, biases\n",
    "\n",
    "#Intitializes random weights and biases based on a given shape for the neural network\n",
    "def init_random_wb(shape):\n",
    "    weights = []\n",
    "    biases= []\n",
    "    for i in range(len(shape)-1):\n",
    "        weights.append(2*(np.random.rand(shape[i+1], shape[i]) - 0.5))\n",
    "        biases.append(2*(np.random.rand(shape[i+1]) - 0.5))\n",
    "    return weights, biases\n",
    "\n",
    "#returns the output-layer of the neural network based on input, weights and biases\n",
    "def make_guess(inp, weights, biases):\n",
    "    guess = inp\n",
    "    for n in range(len(weights)):\n",
    "        guess = sigmoid(weights[n] @ guess + biases[n])\n",
    "    return guess\n",
    "\n",
    "#computes the ideal change in all weights and biases based on one instance of input/output\n",
    "def back_propagation(el_in,el_out, shape, weights, biases):\n",
    "    w_change, b_change = empty_wb(shape)\n",
    "    guess = [el_in]\n",
    "    for j in range(0,len(weights)):\n",
    "        guess.append(sigmoid(weights[j] @ guess[j] + biases[j]))\n",
    "\n",
    "    error = sigmoid_derivative(guess[len(weights)]) * (guess[len(weights)] - el_out)\n",
    "    for j in range(len(weights)-1,-1,-1):\n",
    "        w_change[j]  = np.outer(error, guess[j])\n",
    "        b_change[j]  = error\n",
    "        error =  weights[j].T @ (error)\n",
    "    return w_change, b_change\n",
    "\n",
    "#training the network\n",
    "def train(inp, out, N, shape, alpha, batch_size):\n",
    "    weights, biases = init_random_wb(shape)\n",
    "    w_temp, b_temp = empty_wb(shape)\n",
    "        \n",
    "    for n in range(N):\n",
    "        for i in range(len(inp)):\n",
    "            w_change, b_change = back_propagation(inp[i],out[i], shape, weights, biases)\n",
    "            \n",
    "            w_temp = [wt + wch for wt, wch in zip(w_temp, w_change)]\n",
    "            b_temp = [bt + bch for bt, bch in zip(b_temp, b_change)]\n",
    "            \n",
    "            if i % batch_size == 0:\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] -= alpha *  w_temp[j]/100\n",
    "                    biases[j] -= alpha *  b_temp[j]/100\n",
    "                w_temp, b_temp = empty_wb(shape)\n",
    "    return weights, biases\n",
    "\n",
    "#testing the network\n",
    "def test_network(inp, out, weights, biases, label_basis):\n",
    "    count = 0\n",
    "    distribution = np.zeros(len(label_basis))\n",
    "    for n in range(len(inp)):\n",
    "        guess = np.argmax(make_guess(inp[n], weights, biases))\n",
    "        correct = np.argmax(out[n])\n",
    "        distribution[correct] += 1\n",
    "        if guess == correct:\n",
    "            count += 1\n",
    "    return count / len(inp), distribution / len(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enkel test med lite datasett\n",
    "\n",
    "Her består datasettet kun av 12 lignende og korte setninger som skal klassifiseres i 1 av 3 kategorier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes of training data\n",
    "training_data = []\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how are you?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is it going today?\"})\n",
    "\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"see you later\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"talk to you soon\"})\n",
    "\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"make me a sandwich\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"can you make a sandwich?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"having a sandwich today?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"what's for lunch?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, classes = create_basises(training_data)\n",
    "training, output = vectorize_dataset(training_data,words, classes)\n",
    "weights, biases = train(training, output, 1000, [len(words), 20, len(classes)], 0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0 \n",
      "Distribution:  [0.33333333 0.         0.66666667]\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "test_data.append({\"class\":\"sandwich\", \"sentence\": \"sudo make me a sandwich\"})\n",
    "test_data.append({\"class\":\"sandwich\", \"sentence\": \"make me some lunch\"})\n",
    "test_data.append({\"class\":\"goodbye\", \"sentence\": \"have a nice day\"})\n",
    "\n",
    "test_in, test_out = vectorize_dataset(test_data, words, classes)\n",
    "count, dist = test_network(test_in, test_out, weights, biases, classes)\n",
    "print(\"Precision: \",count, \"\\nDistribution: \", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifiserer anmeldeser fra Amazon som positive eller negative\n",
    "\n",
    "\n",
    "Datasettet er hentet <a href=\"https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235\">herfra</a> og inneholder 10 000 korte anmeldelser merket som positive eller negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how ar you', 'how is yo day', 'good day', 'how is it going today', 'hav a nic day', 'see you lat', 'hav a nic day', 'talk to you soon', 'mak me a sandwich', 'can you mak a sandwich', 'hav a sandwich today', 'whats for lunch']\n",
      "{'how': 7, 'ar': 0, 'you': 23, 'is': 8, 'yo': 22, 'day': 2, 'good': 5, 'it': 9, 'going': 4, 'today': 20, 'hav': 6, 'nic': 14, 'see': 16, 'lat': 10, 'talk': 18, 'to': 19, 'soon': 17, 'mak': 12, 'me': 13, 'sandwich': 15, 'can': 1, 'whats': 21, 'for': 3, 'lunch': 11}\n",
      "(12, 24)\n",
      "['goodbye', 'greeting', 'sandwich']\n",
      "Precision:  0.0 \n",
      "Distribution:  [0.         0.66666667 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "for el in training_data:\n",
    "    text.append(\" \".join(clean_text(el[\"sentence\"])))\n",
    "print(text)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit(text)\n",
    "Y = vectorizer.transform(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "Y = Y.toarray()\n",
    "print(Y.shape)\n",
    "\n",
    "words, classes = create_basises(training_data)\n",
    "training, output = vectorize_dataset(training_data,words, classes)\n",
    "weights, biases = train(Y, output, 1000, [len(Y[0]), 20, len(classes)], 0.5,1)\n",
    "\n",
    "\n",
    "inp1 = \" \".join(clean_text(\"sudo make me a sandwich\"))\n",
    "inp2 = \" \".join(clean_text(\"have a nice day\") )\n",
    "inp3 = \" \".join(clean_text(\"make me some lunch\"))\n",
    "print(classes)\n",
    "inp = np.array([inp1,inp2,inp3])\n",
    "X = (vectorizer.transform(inp)).toarray()\n",
    "out = np.array([[0,1,0], [0,0,1], [0,1,0]])\n",
    "count, dist = test_network(X, out, weights, biases, classes)\n",
    "print(\"Precision: \",count, \"\\nDistribution: \", dist)\n",
    "\n",
    "\n",
    "#IT WORKS HALELUJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laster inn data Fra filen corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "data = open('corpus.txt', encoding = \"utf8\")\n",
    "lines = data.readlines()\n",
    "data.close()\n",
    "\n",
    "#split data into training and testing - lists of dictionaries\n",
    "training_data = []\n",
    "testing_data = []\n",
    "length = len(lines)\n",
    "\n",
    "i = 0\n",
    "for line in lines:\n",
    "    content = line.split()\n",
    "    if i < length * 0.9:\n",
    "        training_data.append({\"class\":content[0], \"sentence\":\" \".join(content[1:])})\n",
    "    else:\n",
    "        testing_data.append({\"class\":content[0], \"sentence\":\" \".join(content[1:])})\n",
    "    i += 1\n",
    "    if i% 1000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gjør om datasettet til vektorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_basis, label_basis = create_basises(training_data)\n",
    "train_data_inp, train_data_out = vectorize_dataset(training_data, word_basis, label_basis)\n",
    "test_data_inp, test_data_out = vectorize_dataset(testing_data, word_basis, label_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trener nettverket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_shape = [len(train_data_inp[0]), 20,20,  len(train_data_out[0])]\n",
    "weights, biases = train(train_data_inp, train_data_out, 10, nn_shape, 0.5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester nettverket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, dist = test_network(test_data_inp, test_data_out, weights, biases, label_basis)\n",
    "print(\"Precision: \",count, \"\\nDistribution: \", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagrer det vektoriserte datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    data_dict = {}\n",
    "    data_dict[\"train_data_inp\"] = train_data_inp\n",
    "    data_dict[\"train_data_out\"] = train_data_out\n",
    "    data_dict[\"test_data_inp\"] = test_data_inp\n",
    "    data_dict[\"test_data_out\"] = test_data_out\n",
    "\n",
    "    with open(datapath+'data_file.pkl', 'wb') as f :\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Henter data fra fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(datapath+'data_file.pkl', 'rb') as f :\n",
    "        data_dict1 = pickle.load(f)\n",
    "\n",
    "    train_data_inp = data_dict1[\"train_data_inp\"]\n",
    "    train_data_out = data_dict1[\"train_data_out\"]\n",
    "    test_data_inp = data_dict1[\"test_data_inp\"]\n",
    "    test_data_out = data_dict1[\"test_data_out\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagrer weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wb():\n",
    "    wb_dict={}\n",
    "    wb_dict[\"w\"]=weights\n",
    "    wb_dict[\"b\"]=biases\n",
    "\n",
    "    with open(datapath+'wb_file.pkl', 'wb') as f :\n",
    "            pickle.dump(wb_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
