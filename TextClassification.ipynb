{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = LancasterStemmer()\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes of training data\n",
    "training_data = []\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how are you?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is it going today?\"})\n",
    "\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"see you later\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"talk to you soon\"})\n",
    "\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"make me a sandwich\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"can you make a sandwich?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"having a sandwich today?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"what's for lunch?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    " \n",
    "def clean_text(t):\n",
    "    t_clean = [stemmer.stem(word) for word in (t.lower()).split()]\n",
    "    for i in range(0, len(t_clean)):\n",
    "        t_clean[i] = ''.join(filter(str.isalnum, t_clean[i]))\n",
    "    return t_clean\n",
    "\n",
    "\n",
    "def create_basises(training_data):\n",
    "    wordvec_basis = []\n",
    "    labels = []\n",
    "    for el in training_data:\n",
    "        labels.append(el[\"class\"])\n",
    "        t_clean = clean_text(el[\"sentence\"])\n",
    "        for w in t_clean:\n",
    "            if w not in stopWords: \n",
    "                wordvec_basis.append(w)\n",
    "    return list(set(wordvec_basis)), list(set(labels))\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow2(sentence, words):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_text(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def bow(t, words):\n",
    "    t_clean = clean_text(t)\n",
    "    t_vec = []\n",
    "    for w in words:\n",
    "        if w in t_clean:\n",
    "            t_vec.append(1)\n",
    "        else:\n",
    "            t_vec.append(0)\n",
    "    t_vec = np.array(t_vec)\n",
    "    return t_vec\n",
    "\n",
    "\n",
    "def make_guess(inp, weights, biases):\n",
    "    guess = inp\n",
    "    for n in range(len(weights)):\n",
    "        guess = sigmoid(weights[n] @ guess + biases[n])\n",
    "    return guess\n",
    "\n",
    "def empty_wb(shape):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(shape)-1):\n",
    "        weights.append(np.zeros((shape[i+1], shape[i])))\n",
    "        biases.append(np.zeros(shape[i+1]))\n",
    "    return weights, biases\n",
    "\n",
    "#Intitializes random weights and biases based on a given shape for the neural network\n",
    "def init_random_wb(shape):\n",
    "    weights = []\n",
    "    biases= []\n",
    "    for i in range(len(shape)-1):\n",
    "        weights.append(2*(np.random.rand(shape[i+1], shape[i]) - 0.5))\n",
    "        biases.append(2*(np.random.rand(shape[i+1]) - 0.5))\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_vec(l, label_basis):\n",
    "    l_vec = np.zeros(len(label_basis))\n",
    "    for i in range(0, len(label_basis)):\n",
    "        if l == label_basis[i]:\n",
    "            l_vec[i] += 1\n",
    "    return l_vec\n",
    "\n",
    "def vectorize_dataset(data, word_basis, label_basis):\n",
    "    text = []\n",
    "    label = []\n",
    "    i = 0\n",
    "    for el in data:\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        t_vec = bow(el[\"sentence\"], word_basis)\n",
    "        l_vec = label_to_vec(el[\"class\"], label_basis)\n",
    "        text.append(t_vec)\n",
    "        label.append(l_vec)\n",
    "    return np.array(text), np.array(label)\n",
    "\n",
    "def test_network(inp, out, weights, biases, label_basis):\n",
    "    count = 0\n",
    "    distribution = np.zeros(len(label_basis))\n",
    "    for n in range(len(inp)):\n",
    "        guess = np.argmax(make_guess(inp[n], weights, biases))\n",
    "        correct = np.argmax(out[n])\n",
    "        distribution[correct] += 1\n",
    "        if guess == correct:\n",
    "            count += 1\n",
    "    return count / len(inp), distribution / len(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, out, N, shape, alpha):\n",
    "    weights, biases = init_random_wb(shape)\n",
    "    w_change, b_change = empty_wb(shape)\n",
    "    guess = [0]*(len(weights) + 1)\n",
    "        \n",
    "    for n in range(N):\n",
    "        print(n)\n",
    "        for i in range(len(inp)):\n",
    "            guess[0] = inp[i]\n",
    "            for j in range(0,len(weights)):\n",
    "                guess[j+1] = sigmoid(weights[j] @ guess[j] + biases[j])\n",
    "\n",
    "            error = sigmoid_derivative(guess[len(weights)]) * (guess[len(weights)] - out[i])\n",
    "            for j in range(len(weights)-1,-1,-1):\n",
    "                w_change[j]  += np.outer(error, guess[j])\n",
    "                b_change[j]  += error\n",
    "                error =  weights[j].T @ (error)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] -= alpha *  w_change[j]/100\n",
    "                    biases[j] -= alpha *  b_change[j]/100\n",
    "                w_change, b_change = empty_wb(shape)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, classes = create_basises(training_data)\n",
    "# create training data\n",
    "training, output = vectorize_dataset(training_data,words, classes)\n",
    "weights, biases = train(training, output, 1000, [len(words), 20, len(classes)], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = bow(\"sudo make me a sandwich\", words)\n",
    "inp2 = bow(\"have a nice day\", words) \n",
    "inp3 = bow(\"make me some lunch\", words)\n",
    "inp = np.array([inp1,inp2,inp3])\n",
    "out = np.array([[0,0,1], [0,1,0], [0,0,1]])\n",
    "count, dist = test_network(inp, out, weights, biases, classes)\n",
    "print(count,\"   \", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('corpus.txt', encoding = \"utf8\")\n",
    "lines = data.readlines()\n",
    "data.close()\n",
    "\n",
    "#split data into training av testing - lists of dictionaries\n",
    "training_data = []\n",
    "testing_data = []\n",
    "length = len(lines)\n",
    "\n",
    "i = 0\n",
    "for line in lines:\n",
    "    content = line.split()\n",
    "    if i < length * 0.9:\n",
    "        training_data.append({\"class\":content[0], \"sentence\":\" \".join(content[1:])})\n",
    "    else:\n",
    "        testing_data.append({\"class\":content[0], \"sentence\":\" \".join(content[1:])})\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_basis, label_basis = create_basises(training_data)\n",
    "train_data_inp, train_data_out = vectorize_dataset(training_data, word_basis, label_basis)\n",
    "test_data_inp, test_data_out = vectorize_dataset(testing_data, word_basis, label_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_shape = [len(word_basis), 20,20,  len(label_basis)]\n",
    "weights, biases = train(train_data_inp[0:4500], train_data_out[0:4500], 10, nn_shape, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, dist = test_network(test_data_inp, test_data_out, weights, biases, label_basis)\n",
    "print(count, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
