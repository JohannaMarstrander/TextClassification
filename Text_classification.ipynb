{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load the dataset\n",
    "data = open('corpus.txt', encoding = \"utf8\")\n",
    "lines = data.readlines()\n",
    "data.close()\n",
    "\n",
    "#split data into training av testing - lists of dictionaries\n",
    "training_data = []\n",
    "testing_data = []\n",
    "length = len(lines)\n",
    "\n",
    "i = 0\n",
    "for line in lines:\n",
    "    content = line.split()\n",
    "    if i < length * 0.9:\n",
    "        training_data.append({\"label\":content[0], \"text\":\" \".join(content[1:])})\n",
    "    else:\n",
    "        testing_data.append({\"label\":content[0], \"text\":\" \".join(content[1:])})\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create basis for vectorizing the text using training data, a list with dictionaries\n",
    "#also creates a list of possible labels\n",
    "stemmer = LancasterStemmer()\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(t):\n",
    "    t_clean = [stemmer.stem(word) for word in (t.lower()).split()]\n",
    "    for i in range(0, len(t_clean)):\n",
    "        t_clean[i] = ''.join(filter(str.isalnum, t_clean[i]))\n",
    "    return t_clean\n",
    "\n",
    "def create_basises(training_data):\n",
    "    wordvec_basis = []\n",
    "    labels = []\n",
    "    for el in training_data:\n",
    "        if el[\"label\"] not in labels:\n",
    "            labels.append(el[\"label\"])\n",
    "        t_clean = clean_text(el[\"text\"])\n",
    "        for w in t_clean:\n",
    "            if w not in wordvec_basis and w not in stopWords: \n",
    "                wordvec_basis.append(w)\n",
    "    return wordvec_basis, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a text and a basis for a vectorizing the text. Cleans text and returns a word_vec according to the given basis\n",
    "def text_to_vec(t, words):\n",
    "    t_clean = clean_text(t)\n",
    "    t_vec = np.zeros(len(words), dtype = \"float\")\n",
    "    for i in range(0,len(words)):\n",
    "        t_vec[i] += t_clean.count(words[i])\n",
    "    t_vec = t_vec/t_vec.sum()\n",
    "    return t_vec\n",
    "\n",
    "def label_to_vec(l, label_basis):\n",
    "    l_vec = np.zeros(len(label_basis))\n",
    "    for i in range(0, len(label_basis)):\n",
    "        if l == label_basis[i]:\n",
    "            l_vec[i] += 1\n",
    "    print(l_vec)\n",
    "    return l_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(data, word_basis, label_basis):\n",
    "    vectorized_data = []\n",
    "    for el in data:\n",
    "        t_vec = text_to_vec(el[\"text\"], word_basis)\n",
    "        l_vec = label_to_vec(el[\"label\"], label_basis)\n",
    "        vectorized_data.append({\"output\":l_vec, \"input\":t_vec})\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Hello, there handsome, you are great!  WOW a a a\"\n",
    "print(clean_text(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_basis, label_basis = create_basises(training_data[0:10])\n",
    "print(word_basis)\n",
    "vec_set= vectorize_dataset(training_data[0:10], word_basis, label_basis)\n",
    "\n",
    "print(training_data[0:2])\n",
    "print(\"\\n\\n\\n\", vec_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function to normalize output\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "#derivative of sigmoid output\n",
    "def sigmoid_output_to_derivative(s_x):\n",
    "    return s_x*(1-s_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
